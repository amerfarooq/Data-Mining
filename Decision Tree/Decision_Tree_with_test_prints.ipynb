{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from sklearn import datasets\n",
    "import math\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Node:\n",
    "  \n",
    "#   def __init__(self, info_gain, purity = 0, class_label = None):\n",
    "#     self.purity = purity\n",
    "#     self.info_gain = info_gain\n",
    "#     self.class_label = class_label\n",
    "#     self.right = None\n",
    "#     self.left = None\n",
    "#     self.level = 0\n",
    "    \n",
    "#   def set_test(self, feat, feat_val, feat_name):\n",
    "#     self.feat = feat\n",
    "#     self.feat_val = feat_val\n",
    "#     self.feat_name = feat_name\n",
    "    \n",
    "#   def get_test(self):    \n",
    "#       return str(self.feat_name) + ' < ' + str(self.feat_val)\n",
    "  \n",
    "#   def is_leaf(self):\n",
    "#     return self.right is None and self.left is None\n",
    "\n",
    "#   def __str__(self):\n",
    "#     if self.is_leaf():\n",
    "#       return str(self.class_label)\n",
    "#     else:\n",
    "#       return self.get_test()\n",
    "  \n",
    "\n",
    "    \n",
    "# class DecisionTree: \n",
    "  \n",
    "#   def __init__(self, data, features = 'all', feature_names = None, purity = 0.95, max_depth = 10, min_samples = 5):\n",
    "#     self.purity = purity\n",
    "#     self.num_features = data.shape[1] - 1\n",
    "#     self.max_depth = max_depth\n",
    "#     self.min_samples = min_samples\n",
    "\n",
    "#     # Use all features of the data if none provided by the user.\n",
    "#     if (features == 'all'):\n",
    "#       self.features = np.arange(0, self.num_features)\n",
    "#     else:\n",
    "#       self.features = features\n",
    "    \n",
    "#     # If feature names not provided, represent them with numbers\n",
    "#     if (feature_names is None):\n",
    "#       self.feature_names = np.arange(0, self.num_features)\n",
    "#     else:\n",
    "#       self.feature_names = feature_names\n",
    "    \n",
    "#     self.root = self._build_tree(data)\n",
    "    \n",
    "\n",
    "#   def _build_tree(self, data, depth=1):\n",
    "#     print (\"partiton shape: \", data.shape)\n",
    "#     # If max depth reached, return most occruing class in data\n",
    "#     if (depth == self.max_depth):\n",
    "#       main_class = self._get_dominant_class(data, check_thresholds=False)\n",
    "#       return Node(class_label = main_class, info_gain=0)\n",
    "      \n",
    "      \n",
    "#     # Else, evaluate other thresholds\n",
    "#     main_class = self._get_dominant_class(data)\n",
    "#     if main_class is not None:\n",
    "#       return Node(class_label = main_class, info_gain=0)\n",
    "  \n",
    "#     else:\n",
    "#       max_info_gain = 0\n",
    "#       best_partition = []\n",
    "      \n",
    "#       for feature in self.features:\n",
    "#         possible_partitions = self._get_possible_partitions(data, feature)\n",
    "#         print (\"\\npossible partitons for feature \", feature, \" \", possible_partitions, \"\\n\")\n",
    "        \n",
    "#         for feat_val in possible_partitions:\n",
    "#           part_a, part_b = self._partition_data(data, feature, feat_val)\n",
    "#           print (\"\\ntesting val: \", feat_val, \" feat: \", feature)\n",
    "#           print (\"part a: \", part_a.shape, \" part b: \", part_b.shape)\n",
    "          \n",
    "#           info_gain = self._evaluate_partitions(data, part_a, part_b)\n",
    "#           print(\"old_max: \", max_info_gain, \" info gain\", info_gain)\n",
    "          \n",
    "#           if (info_gain > max_info_gain):\n",
    "#             print(\"new_max: \", info_gain)\n",
    "#             max_info_gain = info_gain\n",
    "#             best_partition = [feature, feat_val]\n",
    "            \n",
    "#       # This will only occur if the attribute values for the given features are same for all datapoints\n",
    "#       if (len(best_partition) == 0):\n",
    "#         main_class = self._get_dominant_class(data, check_thresholds=False)\n",
    "#         return Node(class_label = main_class, info_gain=0)\n",
    "            \n",
    "#       print (\"best: \", best_partition)\n",
    "#       left, right = self._partition_data(data, best_partition[0], best_partition[1])\n",
    "#       print (\"left \", np.unique(left[:,-1]), \" right: \", np.unique(right[:,-1]))\n",
    "\n",
    "#       node = Node(info_gain = max_info_gain)\n",
    "      \n",
    "#       node.set_test( best_partition[0], best_partition[1], self.feature_names[best_partition[0]] )\n",
    "#       node.left = self._build_tree(left, depth+1)\n",
    "#       node.right = self._build_tree(right, depth+1)\n",
    "      \n",
    "#       return node\n",
    "    \n",
    " \n",
    "#   def print_tree(self):\n",
    "# #     print (self._print(self.root))\n",
    "#     self._preorder_traversal(self.root)  \n",
    "  \n",
    "  \n",
    "#   def _print(self, node, depth=0):\n",
    "#       ret = \"\"\n",
    "\n",
    "#       if node.right is not None:\n",
    "#           ret += self._print(node.right, depth+1)\n",
    "\n",
    "#       ret += \"\\n\" + (\"    \" * depth) + str(node)\n",
    "\n",
    "#       if node.left is not None:\n",
    "#           ret += self._print(node.left, depth+1)\n",
    "\n",
    "#       return ret\n",
    "  \n",
    "  \n",
    "#   def _preorder_traversal(self, node):\n",
    "#     if node.is_leaf():\n",
    "#       print (node.class_label)\n",
    "    \n",
    "#     else:\n",
    "#       print (node.get_test())\n",
    "#       self._preorder_traversal(node.left)\n",
    "#       self._preorder_traversal(node.right)\n",
    "  \n",
    "  \n",
    "#   def predict_point(self, datapoint):\n",
    "#     if datapoint.shape[0] != self.num_features + 1:\n",
    "#       raise RuntimeError(\"Datapoint contains incorrect number of features!\")\n",
    "    \n",
    "#     return self._predict(datapoint)\n",
    "    \n",
    "    \n",
    "#   def predict_set(self, data):\n",
    "#     if data.shape[1] - 1 != self.num_features:\n",
    "#       raise RuntimeError(\"Data contains incorrect number of features!\")\n",
    "      \n",
    "#     predictions = []  \n",
    "    \n",
    "#     for point in data:\n",
    "#       predictions.append(self._predict(point[:-1]))\n",
    "      \n",
    "#     return predictions\n",
    "  \n",
    "  \n",
    "#   def _predict(self, point):\n",
    "#     curr_node = self.root\n",
    "    \n",
    "#     while (True):\n",
    "#       if curr_node.is_leaf():\n",
    "#         return curr_node.class_label\n",
    "\n",
    "#       else:\n",
    "#         if (point[curr_node.feat] < curr_node.feat_val):\n",
    "#           curr_node = curr_node.left\n",
    "        \n",
    "#         else:\n",
    "#           curr_node = curr_node.right\n",
    "  \n",
    "\n",
    "#   def _get_possible_partitions(self, data, feature):\n",
    "#     \"\"\"\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#       data (np array):  Contains all datapoints with their feature values\n",
    "#       feature (int):    A column index of the given data that corresponds to the feature on\n",
    "#                         which to partiton the data\n",
    "                        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#       list: Possible values of the given feature on which the given data can be partitioned. Does not\n",
    "#             contain any duplicates\n",
    "#     \"\"\"\n",
    "#     feat_vals = np.sort(data[:, feature])\n",
    "#     return np.unique((feat_vals[1:] + feat_vals[:-1]) / 2)\n",
    "      \n",
    "    \n",
    "#   def _partition_data(self, data, feature, feature_val):\n",
    "#     less_than = data[data[:, feature] < feature_val]\n",
    "#     greater_than = data[data[:, feature] >= feature_val]\n",
    "    \n",
    "#     return less_than, greater_than\n",
    "    \n",
    "    \n",
    "#   def _evaluate_partitions(self, data, part_a, part_b, metric = 'ent'):\n",
    "#     if metric == 'ent':\n",
    "#       return self._get_info_gain(data, part_a, part_b)\n",
    "    \n",
    "\n",
    "#   def _get_info_gain(self, data, part_a, part_b):\n",
    "#     \"\"\"\n",
    "#     Returns the information gain if data partitioned into part_a and part_b\n",
    "    \n",
    "#     info gain = ent_parent - [(p_a * ent_a) + (p_b * ent_b)]\n",
    "    \n",
    "#     p_a: probability of class a datapoints in data\n",
    "#     p_b: probability of class b datapoints in data\n",
    "#     ent_a: entropy of partition a\n",
    "#     ent_b: entropy of partition b\n",
    "    \n",
    "#     \"\"\"\n",
    "#     total_size = data.shape[0]\n",
    "#     part_a_size = part_a.shape[0]\n",
    "#     part_b_size = part_b.shape[0]\n",
    "    \n",
    "#     parent_ent = self._get_entropy(data)\n",
    "#     part_a_ent = self._get_entropy(part_a)\n",
    "#     part_b_ent = self._get_entropy(part_b)\n",
    "    \n",
    "#     return parent_ent - (((part_a_size / total_size) * part_a_ent) + ((part_b_size / total_size) * part_b_ent))\n",
    "\n",
    "  \n",
    "#   def _get_entropy(self, partition):\n",
    "#     value, counts = np.unique(partition[:,-1], return_counts=True)\n",
    "#     return entropy(counts, base=2)\n",
    "  \n",
    "  \n",
    "#   def _get_dominant_class(self, data, check_thresholds=True):\n",
    "#     \"\"\"\n",
    "#     Evaluates whether the partiton 'data' will be split further based on provided thresholds. If not,\n",
    "#     return the dominant class of the partition.\n",
    "    \n",
    "#     Thresholds:\n",
    "#     -----------\n",
    "#     1- If tree has reached its max_depth\n",
    "#     2- If purity of most occuring class is greater than the purity threshold\n",
    "#     3- If no.of datapoints in partition is less then min_samples threshold\n",
    "    \n",
    "#     \"\"\"\n",
    "#     classes, counts = np.unique(data[:, -1], return_counts=True)\n",
    "   \n",
    "#     total_samples = np.sum(counts)\n",
    "#     dominant_class_purity = np.amax(counts) / total_samples\n",
    "    \n",
    "#     if (not check_thresholds or (total_samples <= self.min_samples) or (dominant_class_purity >= self.purity)):\n",
    "#       i, = np.where(counts == np.amax(counts))\n",
    "#       return classes[i][0]\n",
    "    \n",
    "#     return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "  \n",
    "  def __init__(self, info_gain, purity = 0, class_label = None):\n",
    "    self.purity = purity\n",
    "    self.info_gain = info_gain\n",
    "    self.class_label = class_label\n",
    "    self.right = None\n",
    "    self.left = None\n",
    "    self.level = 0\n",
    "    \n",
    "  def set_test(self, feat, feat_val, feat_name):\n",
    "    self.feat = feat\n",
    "    self.feat_val = feat_val\n",
    "    self.feat_name = feat_name\n",
    "    \n",
    "  def get_test(self):    \n",
    "      return str(self.feat_name) + ' < ' + str(self.feat_val)\n",
    "  \n",
    "  def is_leaf(self):\n",
    "    return self.right is None and self.left is None\n",
    "\n",
    "  def __str__(self):\n",
    "    if self.is_leaf():\n",
    "      return str(self.class_label)\n",
    "    else:\n",
    "      return self.get_test()\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "class DecisionTree:\n",
    "  \n",
    "  def __init__(self, data, features = 'all', feature_names = None, \n",
    "               purity = 0.95, max_depth = 10, min_samples = 5, metric = 'entropy', show_results=False):\n",
    "    \n",
    "    self.purity = purity\n",
    "    self.max_depth = max_depth\n",
    "    self.min_samples = min_samples\n",
    "    self.metric = metric\n",
    "    self.show_results = show_results\n",
    "\n",
    "    # Use all features of the data if none provided by the user.\n",
    "    if (features == 'all'):\n",
    "      self.features = np.arange(0, data.shape[1] - 1)\n",
    "    else:\n",
    "      self.features = features\n",
    "      \n",
    "    self.num_features = self.features.shape[0]\n",
    "\n",
    "    # If feature names not provided, represent them with numbers\n",
    "    if (feature_names is None):\n",
    "      self.feature_names = np.arange(0, data.shape[1])\n",
    "    else:\n",
    "      self.feature_names = feature_names\n",
    "    \n",
    "    self.root = self._build_tree(data)\n",
    "    \n",
    "\n",
    "  def _build_tree(self, data, depth=1):\n",
    "    # If max depth reached, return most frequent class in the data\n",
    "    if (depth == self.max_depth):\n",
    "      main_class = self._get_dominant_class(data, check_thresholds=False)\n",
    "      if self.show_results: print (\"Max depth reached. Leaf node of \", main_class)\n",
    "      return Node(class_label = main_class, info_gain=0)\n",
    "      \n",
    "      \n",
    "    # Else, evaluate other thresholds\n",
    "    main_class = self._get_dominant_class(data, check_thresholds=True)\n",
    "    if main_class is not None:\n",
    "      if self.show_results:\n",
    "        print (\"Some threshold reached. Leaf node of \", main_class)\n",
    "        print (\"Partition: \" + str(data.shape) + \"\\n\")\n",
    "        print(data)\n",
    "        print(\"\\n\")\n",
    "      return Node(class_label = main_class, info_gain=0)\n",
    "  \n",
    "    else:\n",
    "      max_info_gain = 0\n",
    "      best_partition = []\n",
    "      \n",
    "      for feature in self.features:\n",
    "        possible_partitions = self._get_possible_partitions(data, feature)\n",
    "        \n",
    "        for feat_val in possible_partitions:\n",
    "          part_a, part_b = self._partition_data(data, feature, feat_val)\n",
    "          \n",
    "          info_gain = self._evaluate_partitions(data, part_a, part_b)\n",
    "          \n",
    "          if (info_gain > max_info_gain):\n",
    "            max_info_gain = info_gain\n",
    "            best_partition = [feature, feat_val]\n",
    "            \n",
    "      if self.show_results: print (\"\\nBest partition: \", best_partition)\n",
    "      # This will only occur if the attribute values for the given features are same for all datapoints\n",
    "      if (len(best_partition) == 0):\n",
    "        main_class = self._get_dominant_class(data, check_thresholds=False)\n",
    "        return Node(class_label = main_class, info_gain=0)\n",
    "            \n",
    "      left, right = self._partition_data(data, best_partition[0], best_partition[1])\n",
    "      \n",
    "      if self.show_results:\n",
    "        print(\"Left partiton: \", str(left.shape))\n",
    "        print(left)\n",
    "        print(\"\\nRight partiton: \", str(right.shape))\n",
    "        print(right)\n",
    "      \n",
    "      node = Node(info_gain = max_info_gain)\n",
    "      \n",
    "      node.set_test( best_partition[0], best_partition[1], self.feature_names[best_partition[0]] )\n",
    "      node.left = self._build_tree(left, depth+1)\n",
    "      node.right = self._build_tree(right, depth+1)\n",
    "      \n",
    "      return node\n",
    "    \n",
    "    \n",
    "  def _get_dominant_class(self, data, check_thresholds=True):\n",
    "    \"\"\"\n",
    "    Evaluates whether the partiton 'data' will be split further based on provided thresholds. \n",
    "    \n",
    "    Thresholds:\n",
    "    -----------\n",
    "    1- If tree has reached its max_depth\n",
    "    2- If purity of most occuring class in a partition is greater than the purity threshold\n",
    "    3- If no.of datapoints in partition is less then min_samples threshold\n",
    "    \n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(data[:, -1], return_counts=True)\n",
    "    \n",
    "    total_samples = np.sum(counts)\n",
    "    dominant_class_purity = np.amax(counts) / total_samples\n",
    "    \n",
    "    if (not check_thresholds or (total_samples <= self.min_samples) or (dominant_class_purity >= self.purity)):\n",
    "      i, = np.where(counts == np.amax(counts))\n",
    "      return classes[i][0]\n",
    "    \n",
    "    return None  \n",
    "    \n",
    "    \n",
    " \n",
    "  def print_tree(self):\n",
    "    self._preorder_traversal(self.root)  \n",
    "  \n",
    "   \n",
    "  def _preorder_traversal(self, node):\n",
    "    if node.is_leaf():\n",
    "      print (node.class_label)\n",
    "    \n",
    "    else:\n",
    "      print (node.get_test())\n",
    "      self._preorder_traversal(node.left)\n",
    "      self._preorder_traversal(node.right)\n",
    "  \n",
    "  \n",
    "  def predict_point(self, datapoint):\n",
    "    if datapoint[self.features].shape[0] != self.num_features:\n",
    "      raise RuntimeError(\"Datapoint contains incorrect number of features!\")\n",
    "    \n",
    "    return self._predict(datapoint)\n",
    "    \n",
    "    \n",
    "  def predict_set(self, data):\n",
    "    if data.shape[1] - 1 != self.num_features:\n",
    "      raise RuntimeError(\"Data contains incorrect number of features!\")\n",
    "      \n",
    "    predictions = []  \n",
    "    \n",
    "    for point in data:\n",
    "      predictions.append(self._predict(point[:-1]))\n",
    "      \n",
    "    return predictions\n",
    "  \n",
    "  \n",
    "  def _predict(self, point):\n",
    "    curr_node = self.root\n",
    "    \n",
    "    while (True):\n",
    "      if curr_node.is_leaf():\n",
    "        return curr_node.class_label\n",
    "\n",
    "      else:\n",
    "        if (point[curr_node.feat] < curr_node.feat_val):\n",
    "          curr_node = curr_node.left\n",
    "        \n",
    "        else:\n",
    "          curr_node = curr_node.right\n",
    "  \n",
    "\n",
    "  def _get_possible_partitions(self, data, feature):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "      data (np array):  Contains all datapoints with their feature values\n",
    "      feature (int):    A column index of the given data that corresponds to the feature on\n",
    "                        which to partiton the data\n",
    "                        \n",
    "    Returns:\n",
    "    --------\n",
    "      list: Possible values of the given feature on which the given data can be partitioned. Does not\n",
    "            contain any duplicates\n",
    "    \"\"\"\n",
    "    feat_vals = np.sort(data[:, feature])\n",
    "    return np.unique((feat_vals[1:] + feat_vals[:-1]) / 2)\n",
    "      \n",
    "    \n",
    "  def _partition_data(self, data, feature, feature_val):\n",
    "    less_than = data[data[:, feature] < feature_val]\n",
    "    greater_than = data[data[:, feature] >= feature_val]\n",
    "    \n",
    "    return less_than, greater_than\n",
    "    \n",
    "    \n",
    "  def _evaluate_partitions(self, data, part_a, part_b):\n",
    "    if self.metric == 'entropy':\n",
    "      return self._get_info_gain(data, part_a, part_b)\n",
    "    \n",
    "    elif self.metric == 'gini':\n",
    "      return self._get_gini_index(data, part_a, part_b)\n",
    "\n",
    "    else:\n",
    "      raise RuntimeError(\"Invalid metric provided\")\n",
    "    \n",
    "    \n",
    "  def _get_info_gain(self, data, part_a, part_b):\n",
    "    \"\"\"\n",
    "    Returns the information gain if data partitioned into part_a and part_b\n",
    "    \n",
    "    info gain = ent_parent - [(p_a * ent_a) + (p_b * ent_b)]\n",
    "    \n",
    "    p_a: probability of class a datapoints in data\n",
    "    p_b: probability of class b datapoints in data\n",
    "    ent_a: entropy of partition a\n",
    "    ent_b: entropy of partition b\n",
    "    \n",
    "    \"\"\"\n",
    "    total_size = data.shape[0]\n",
    "    part_a_size = part_a.shape[0]\n",
    "    part_b_size = part_b.shape[0]\n",
    "    \n",
    "    parent_ent = self._get_entropy(data)\n",
    "    part_a_ent = self._get_entropy(part_a)\n",
    "    part_b_ent = self._get_entropy(part_b)\n",
    "    \n",
    "    return parent_ent - (((part_a_size / total_size) * part_a_ent) + ((part_b_size / total_size) * part_b_ent))\n",
    "\n",
    "  \n",
    "  def _get_gini_index(self, data, part_a, part_b):\n",
    "    \"\"\"\n",
    "    Returns the gini index if data partitioned into part_a and part_b\n",
    "    \n",
    "    gini index = gini_parent - [(p_a * gini_a) + (p_b * gini_b)]\n",
    "    \n",
    "    p_a: probability of class a datapoints in data\n",
    "    p_b: probability of class b datapoints in data\n",
    "    gini_a: gini index of partition a\n",
    "    gini_b: gini index of partition b\n",
    "    \n",
    "    \"\"\"\n",
    "    total_size = data.shape[0]\n",
    "    part_a_size = part_a.shape[0]\n",
    "    part_b_size = part_b.shape[0]\n",
    "    \n",
    "    gini_parent = self._get_gini(data)\n",
    "    gini_a = self._get_gini(part_a)\n",
    "    gini_b = self._get_gini(part_b)\n",
    "    \n",
    "    return gini_parent - (((part_a_size / total_size) * gini_a) + ((part_b_size / total_size) * gini_b))\n",
    "  \n",
    "  \n",
    "  def _get_entropy(self, partition):\n",
    "    values, counts = np.unique(partition[:,-1], return_counts=True)\n",
    "    return entropy(counts, base=2)\n",
    "  \n",
    "  \n",
    "  def _get_gini(self, partition):\n",
    "    values, counts = np.unique(partition[:,-1], return_counts=True)\n",
    "    total_points = np.sum(counts)\n",
    "    \n",
    "    total = 0\n",
    "    for point in counts:\n",
    "      total += (point / total_points)**2\n",
    "      \n",
    "    return 1 - total\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = datasets.load_wine()\n",
    "data = np.array(data_set[\"data\"])\n",
    "targets = np.array(data_set[\"target\"]).reshape(-1, 1)\n",
    "final_dataset = np.append(data, targets, axis=1)\n",
    "np.random.shuffle(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split and accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_percent = 0.8):\n",
    "  split = int(split_percent * data.shape[0])\n",
    "  return data[:split], data[split:]\n",
    "\n",
    "\n",
    "def get_accuracy(test, tree):\n",
    "  num_test_points = test.shape[0]\n",
    "  correct = 0\n",
    "  \n",
    "  for point in test:  \n",
    "    if (math.isclose(point[-1], tree.predict_point(point))):\n",
    "      correct += 1\n",
    "      \n",
    "  print (\"\\nCorrect: \" + str(correct), \" |Wrong: \" + str(num_test_points - correct), \" |Total: \" + str(num_test_points), \" |Accuracy: \" + str(correct / float(num_test_points)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying across features and classes of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datapoints:  178\n",
      "Total features:  13\n",
      "Total classes:  3\n",
      "\n",
      "-----------------------------\n",
      " Varying number of features:\n",
      "-----------------------------\n",
      "\n",
      "•  2  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 25  |Wrong: 11  |Total: 36  |Accuracy: 0.6944444444444444\n",
      "\n",
      "\n",
      "•  3  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 24  |Wrong: 12  |Total: 36  |Accuracy: 0.6666666666666666\n",
      "\n",
      "\n",
      "•  4  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 27  |Wrong: 9  |Total: 36  |Accuracy: 0.75\n",
      "\n",
      "\n",
      "•  5  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 26  |Wrong: 10  |Total: 36  |Accuracy: 0.7222222222222222\n",
      "\n",
      "\n",
      "•  6  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 29  |Wrong: 7  |Total: 36  |Accuracy: 0.8055555555555556\n",
      "\n",
      "\n",
      "•  7  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  8  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  9  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  10  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  11  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  12  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  13  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 31  |Wrong: 5  |Total: 36  |Accuracy: 0.8611111111111112\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------\n",
      " Varying number of classes:\n",
      "-----------------------------\n",
      "\n",
      "•  13  features,  2  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 33  |Wrong: 3  |Total: 36  |Accuracy: 0.9166666666666666\n",
      "\n",
      "\n",
      "•  13  features,  3  classes\n",
      "-----------------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 31  |Wrong: 5  |Total: 36  |Accuracy: 0.8611111111111112\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_classes = np.unique(targets).shape[0]\n",
    "total_features = final_dataset.shape[1] - 1\n",
    "\n",
    "print(\"Total datapoints: \", final_dataset.shape[0])\n",
    "print(\"Total features: \", final_dataset.shape[1] - 1)\n",
    "print(\"Total classes: \", total_classes)\n",
    "\n",
    "\n",
    "print(\"\\n-----------------------------\")\n",
    "print(\" Varying number of features:\")\n",
    "print(\"-----------------------------\\n\")\n",
    "\n",
    "for i in range(2, total_features + 1):\n",
    "  print(\"• \", i, \" features, \", total_classes, \" classes\")\n",
    "  print(\"-----------------------------\\n\")\n",
    "  \n",
    "  set_a = np.copy(final_dataset)\n",
    "\n",
    "  train_a, test_a = split_data(set_a)\n",
    "  print (\"Training datasize: \", train_a.shape)\n",
    "  print (\"Test datasize: \", test_a.shape)\n",
    "\n",
    "  tree_a = DecisionTree(train_a, features=np.arange(0, i), metric='gini')\n",
    "\n",
    "  get_accuracy(test_a, tree_a) \n",
    "  print('\\n')\n",
    "  \n",
    "\n",
    "  \n",
    "\n",
    "  \n",
    "print(\"\\n\\n-----------------------------\")\n",
    "print(\" Varying number of classes:\")\n",
    "print(\"-----------------------------\\n\")\n",
    "\n",
    "for j in range(2, total_classes + 1):\n",
    "  print(\"• \", total_features, \" features, \", j, \" classes\")\n",
    "  print(\"-----------------------------\\n\")\n",
    "  \n",
    "  set_b = np.copy(final_dataset)\n",
    "\n",
    "  train_b, test_b = split_data(set_b)\n",
    "  print (\"Training datasize: \", train_b.shape)\n",
    "  print (\"Test datasize: \", test_b.shape)\n",
    "\n",
    "  allowed_classes = np.arange(0, j)\n",
    "  for point in set_b:\n",
    "    if point[-1] not in allowed_classes:\n",
    "      point[-1] = allowed_classes[-1]\n",
    "    \n",
    "  tree_b = DecisionTree(train_b, features='all', metric='gini')\n",
    "\n",
    "  get_accuracy(test_b, tree_b) \n",
    "  print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features:  [0 3]\n",
      "Using classes:  3\n",
      "----------------------\n",
      "\n",
      "Training datasize:  (142, 14)\n",
      "Test datasize:  (36, 14)\n",
      "\n",
      "Correct: 28  |Wrong: 8  |Total: 36  |Accuracy: 0.7777777777777778\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_features = np.asarray([0, 3])\n",
    "c_classes = 3\n",
    "\n",
    "print(\"Using features: \", c_features)\n",
    "print(\"Using classes: \", c_classes)\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "set_c = np.copy(final_dataset)\n",
    "\n",
    "train_c, test_c = split_data(set_c)\n",
    "print (\"Training datasize: \", train_c.shape)\n",
    "print (\"Test datasize: \", test_c.shape)\n",
    "\n",
    "allowed_classes = np.arange(2, c_classes + 1)\n",
    "for point in set_c:\n",
    "  if point[-1] not in allowed_classes:\n",
    "    point[-1] = allowed_classes[-1]\n",
    "\n",
    "tree_c = DecisionTree(train_c, features=c_features,max_depth=10,min_samples=5,purity=0.95, metric='gini',show_results=False)\n",
    "\n",
    "get_accuracy(test_c, tree_c) \n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
